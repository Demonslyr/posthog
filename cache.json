[
    {
        "path": "products/llm_observability/__init__.py",
        "summary": ""
    },
    {
        "path": "products/llm_observability/manifest.tsx",
        "summary": "```json\n{\n  \"productName\": \"LLM Observability\",\n  \"features\": [\n    {\n      \"name\": \"LLM Observability Dashboard\",\n      \"description\": \"Provides a central dashboard for monitoring and analyzing LLM performance and usage. Accessible via the /llm-observability route.\"\n    },\n    {\n      \"name\": \"LLM Generations View\",\n      \"description\": \"Allows users to view and analyze LLM generations. Accessible via the /llm-observability/generations route.\"\n    },\n    {\n      \"name\": \"LLM Traces View\",\n      \"description\": \"Enables users to explore and analyze LLM traces for debugging and performance optimization. Accessible via the /llm-observability/traces route.\"\n    },\n    {\n      \"name\": \"LLM Trace Details View\",\n      \"description\": \"Provides detailed information about a specific LLM trace, including events and timestamps. Accessible via the /llm-observability/traces/:id route.\"\n    },\n    {\n      \"name\": \"LLM Users View\",\n      \"description\": \"Allows users to view and analyze LLM usage by different users. Accessible via the /llm-observability/users route.\"\n    }\n  ]\n}\n```"
    },
    {
        "path": "products/llm_observability/frontend/ConversationDisplay/ConversationDisplay.tsx",
        "summary": "This code defines a `ConversationDisplay` component in React, designed to display a conversation between a user and an LLM (Large Language Model). It receives `eventProperties` as input, which contains information about the conversation. The component renders a header with metadata about the LLM interaction and then displays the conversation messages.\n\nHere's a breakdown of the product features based on the code:\n\n**Features:**\n\n*   **LLM Interaction Metadata Display:** Shows key metrics about the LLM interaction, including input tokens, output tokens, total cost (USD), model name, and latency. This allows users to quickly understand the resource usage and performance of the LLM.\n*   **Conversation Message Display:** Presents the input, output, and tools used in the conversation. It handles different output formats, including choices, standard output, and error messages. This provides a clear view of the conversation flow.\n*   **Error Handling Indication:** Indicates if an error occurred during the LLM interaction, providing a visual cue for failed requests.\n*   **HTTP Status Code Display:** Shows the HTTP status code of the LLM request, aiding in debugging and understanding the success or failure of the interaction.\n"
    },
    {
        "path": "products/llm_observability/frontend/ConversationDisplay/MetadataHeader.tsx",
        "summary": "This code defines a React component called `MetadataHeader` that displays metadata related to a Language Model (LLM) conversation or generation. It shows information like latency, token usage, model name, and cost. The component uses `MetadataTag` and `LemonTag` components for displaying the information in a structured and visually appealing way.\n\nHere's a breakdown of the features:\n\n*   **Error Indicator:** Displays an error tag if the `isError` prop is true.\n*   **Latency Display:** Shows the latency of the LLM interaction in seconds, rounded to two decimal places.\n*   **Token Usage Display:** Presents the number of input (prompt) and output (completion) tokens, along with their sum.\n*   **Model Name Display:** Shows the name of the LLM model used, and allows the user to copy the model name.\n*   **Total Cost Display:** Displays the total cost of the LLM generation in USD, rounded to six decimal places.\n*   **Flexible Layout:** Uses `flex flex-wrap` to ensure the metadata tags wrap to the next line on smaller screens.\n*   **Customizable Styling:** Accepts a `className` prop to allow for custom styling.\n"
    },
    {
        "path": "products/llm_observability/frontend/components/FeedbackTag.tsx",
        "summary": "## Code Summary:\n\nThe `FeedbackTag.tsx` file defines a React component called `FeedbackTag`. This component is designed to display user feedback associated with a specific item (likely an LLM interaction). It uses the `LemonTag` component from `@posthog/lemon-ui` to render a tag-like element. The component receives feedback text as a property (`properties`) and displays a preview of the feedback within the tag. A `CopyToClipboardInline` component allows users to easily copy the full feedback text to their clipboard. If no feedback is provided, it displays \"No feedback provided\".\n\n## Product Features:\n\nHere's a breakdown of the features provided by the `FeedbackTag` component:\n\n*   **Feedback Display:** Shows a concise representation of user feedback within a tag. This allows for quick identification of items with associated feedback.\n*   **Feedback Preview:** Displays a short preview of the feedback text directly within the tag, giving users a glimpse of the content.\n*   **Full Feedback Access:** Provides a tooltip that displays the complete feedback text when hovering over the tag.\n*   **Copy to Clipboard:** Enables users to easily copy the full feedback text to their clipboard for further use or analysis.\n*   **No Feedback Indication:** Clearly indicates when no feedback is available for a particular item.\n"
    },
    {
        "path": "products/llm_observability/frontend/components/MetadataTag.tsx",
        "summary": "This code defines a React component called `MetadataTag`. It's a tag-like component that displays metadata, providing either a tooltip or a copy-to-clipboard functionality based on whether a `textToCopy` prop is provided. It leverages the `LemonTag` component from `@posthog/lemon-ui` for styling and the `CopyToClipboardInline` component for the copy functionality.\n\nHere's a breakdown of the product features:\n\n*   **Metadata Display:** Displays metadata in a visually distinct tag format.\n*   **Tooltip on Hover:** Shows a tooltip with the label when the user hovers over the tag, providing additional context.\n*   **Copy to Clipboard:**  Allows users to copy the metadata value to their clipboard with a single click.  The `textToCopy` prop determines the value that is copied.\n*   **Visual Styling:** Uses `LemonTag` for consistent styling with the PostHog Lemon UI library.\n"
    },
    {
        "path": "products/llm_observability/frontend/llmObservabilityTraceDataLogic.ts",
        "summary": "## Code Summary\n\nThis code defines a Kea logic `llmObservabilityTraceDataLogic` that manages and processes data related to LLM observability traces. It fetches trace data, filters events, and structures the data for display in a tree-like format. It uses `dataNodeLogic` to handle data fetching and caching. It also defines selectors to extract specific information from the trace data, such as showable events, metric events, feedback events, and a combined list of metrics and feedback. The code also includes a `restoreTree` function that reconstructs the trace event hierarchy from a flat list of events, handling potential circular references.\n\n## Product Features\n\nHere's a breakdown of the features provided by this code, as it relates to LLM Observability:\n\n*   **Trace Data Fetching and Caching:** Fetches LLM trace data based on a trace ID and caches the results for performance.\n*   **Event Filtering:** Filters trace events to separate showable events (excluding feedback events), metric events, and feedback events.\n*   **Metric and Feedback Aggregation:** Combines metric and feedback events into a single list for easy display and analysis.\n*   **Trace Event Hierarchy Reconstruction:** Reconstructs the hierarchical structure of trace events, allowing for a tree-like visualization of the trace.\n*   **Circular Reference Detection:** Detects and handles circular references in the trace event hierarchy to prevent infinite loops.\n*   **Event Selection:** Allows selecting a specific event within the trace for detailed inspection.\n"
    },
    {
        "path": "products/llm_observability/frontend/llmObservabilityTraceLogic.ts",
        "summary": "## Code Summary\n\nThis code defines a Kea logic module called `llmObservabilityTraceLogic`. This logic is responsible for managing the state and behavior of a trace view within an LLM observability dashboard. It handles setting the trace ID, event ID, and date range, constructing a query for fetching trace data, and generating breadcrumbs for navigation. It also uses `kea-router` to connect the trace view URL to the logic, allowing the trace ID, event ID, and timestamp to be extracted from the URL and used to update the logic's state.\n\n## Product Features\n\nHere's a breakdown of the features provided by this code:\n\n*   **Trace ID Management:** Allows setting and storing the ID of the trace being viewed. This ID is used to fetch the relevant trace data.\n*   **Event ID Management:** Allows setting and storing the ID of a specific event within the trace.\n*   **Date Range Filtering:** Enables filtering traces based on a date range, allowing users to focus on specific time periods.\n*   **Trace Data Query Construction:** Dynamically constructs a query to fetch trace data based on the trace ID and date range. This query is used to retrieve the trace information from the backend.\n*   **Breadcrumb Generation:** Creates breadcrumbs for easy navigation within the LLM observability dashboard, allowing users to quickly move between the dashboard, traces list, and specific trace views.\n*   **URL-Based State Management:** Integrates with the URL to manage the trace ID, event ID, and timestamp, allowing users to share and bookmark specific trace views.\n"
    },
    {
        "path": "products/llm_observability/frontend/LLMInputOutput.tsx",
        "summary": "## Summary of `LLMInputOutput.tsx`\n\nThis React component, `LLMInputOutput`, is designed to display the input and output of a Large Language Model (LLM). It takes JSX elements as input and output, along with optional headings and a boolean to control whether the component has a border. It renders the input and output sections with appropriate headings and icons, separated by a divider.\n\n## Product Features of `LLMInputOutput` Component\n\nHere's a breakdown of the features offered by the `LLMInputOutput` component:\n\n*   **Input Display:** Renders the input provided to the LLM. This allows users to see exactly what was sent to the model.\n*   **Output Display:** Renders the output generated by the LLM. This displays the model's response to the given input.\n*   **Customizable Headings:** Allows users to specify custom headings for the input and output sections, providing context and clarity. Defaults to \"Input\" and \"Output\".\n*   **Visual Separation:** Uses a `LemonDivider` component to visually separate the `LLMInputOutput` component from other content on the page.\n*   **Bordered Option:** Provides an option to display the input and output within a bordered container, improving visual grouping and distinction.\n*   **Clear Visual Cues:** Uses up and down arrow icons to visually represent input and output, respectively, enhancing user understanding.\n"
    },
    {
        "path": "products/llm_observability/frontend/LLMObservabilityReloadAction.tsx",
        "summary": "This code defines a React component, `LLMObservabilityReloadAction`, which provides a button to refresh the LLM Observability dashboard items. It uses the `llmObservabilityLogic` (likely a Kea logic hook) to manage the refreshing state and trigger the refresh action. The button displays a spinner while refreshing and shows the last refresh time otherwise.\n\nHere's a breakdown of the product features based on the code:\n\n**Features:**\n\n*   **Dashboard Refresh:** Allows users to manually refresh the data displayed on the LLM Observability dashboard.\n    *   Description: Provides a button that, when clicked, triggers a refresh of all the data shown in the LLM Observability dashboard. This ensures users have the most up-to-date information.\n*   **Refreshing Indicator:**  Visually indicates when the dashboard is actively refreshing data.\n    *   Description:  Displays a spinner icon and \"Refreshing...\" text on the refresh button while the data is being updated, providing feedback to the user that the refresh action is in progress.\n*   **Last Refresh Timestamp:** Displays the time since the last successful data refresh.\n    *   Description: Shows how long ago the data was last updated, giving users context on the freshness of the information they are viewing. The timestamp is dynamically updated to show a \"time ago\" format (e.g., \"Last updated 5 minutes ago\").\n*   **Button Disabling:** Disables the refresh button while a refresh is in progress.\n    *   Description: Prevents users from initiating multiple refresh requests simultaneously, which could potentially overload the system or lead to inconsistent data. The button is disabled and shows \"Refreshing...\" as the disabled reason.\n"
    },
    {
        "path": "products/llm_observability/frontend/LLMObservabilityUsers.tsx",
        "summary": "This code defines a React component, `LLMObservabilityUsers`, which displays a table of users related to LLM observability data. It fetches user data using a HogQL query and presents it in a `DataTable` component. The table includes information about each user, such as their distinct ID, first seen date, last seen date, number of traces, number of generations, and total cost.  The component also allows filtering of users based on date ranges, test accounts, and properties. Clicking on a user navigates to the LLM Observability Traces page, filtered to show traces for that specific user.\n\nHere's a breakdown of the features:\n\n*   **User Table Display:** Shows a table of users with key information related to LLM usage.\n*   **User Details:** Displays distinct ID, first seen date, last seen date, traces count, generations count, and total cost for each user.\n*   **Data Fetching with HogQL:** Fetches user data using a HogQL query, enabling flexible and powerful data retrieval.\n*   **Filtering:** Supports filtering users by date range, test accounts, and user properties.\n*   **Navigation to User Traces:** Clicking on a user navigates to a filtered view of LLM Observability Traces, showing only traces associated with that user.\n*   **Cost Display:** Displays the total cost associated with each user's LLM usage in USD, formatted to four decimal places.\n"
    },
    {
        "path": "products/llm_observability/frontend/components/MetricTag.tsx",
        "summary": "## Summary of `MetricTag.tsx`\n\nThis React component, `MetricTag`, is designed to display LLM observability metrics in a user-friendly tag format. It takes a `properties` object as input, extracts the metric name (`$ai_metric_name`) and value (`$ai_metric_value`), and renders them within a LemonTag (from PostHog's Lemon UI library). The component handles long metric values by truncating them and providing a tooltip with the full value. It also includes a \"copy to clipboard\" functionality for the metric value. The `identifierToHuman` function is used to convert the metric name into a more readable format.\n\n## Product Features\n\nHere's a breakdown of the features provided by the `MetricTag` component:\n\n*   **Metric Display:** Shows the name and value of an LLM observability metric.\n    *   *Description:* Presents key metrics in a concise and easily digestible format.\n*   **Human-Readable Metric Names:** Converts technical metric identifiers into human-readable labels.\n    *   *Description:* Improves user understanding by displaying metric names in a more intuitive way.\n*   **Value Truncation:** Handles long metric values by truncating them for display.\n    *   *Description:* Prevents long values from disrupting the layout and provides a cleaner user interface.\n*   **Tooltip with Full Value:** Displays the full metric value in a tooltip when the value is truncated.\n    *   *Description:* Allows users to access the complete metric value when needed.\n*   **Copy to Clipboard:** Enables users to easily copy the metric value to their clipboard.\n    *   *Description:* Simplifies the process of using the metric value in other applications or analyses.\n*   **Lemon UI Integration:** Uses PostHog's Lemon UI library for consistent styling and theming.\n    *   *Description:* Ensures a cohesive look and feel within the PostHog platform.\n"
    },
    {
        "path": "products/llm_observability/frontend/LLMObservabilityTracesScene.tsx",
        "summary": "## Code Summary\n\nThis code defines a React component called `LLMObservabilityTraces` that displays a table of LLM (Large Language Model) traces. It uses the `DataTable` component to render the data and interacts with the `llmObservabilityLogic` (Kea logic) to manage the query and data. The table displays information such as ID, timestamp, trace name, person, latency, token usage, and total cost for each trace. Each column has a custom rendering component to format the data appropriately. The component also allows users to filter and modify the traces query.\n\n## Product Features\n\nHere's a list of features provided by the `LLMObservabilityTraces` component:\n\n*   **LLM Trace Visualization:** Displays LLM traces in a tabular format, providing a clear overview of individual requests and their associated data.\n*   **Filtering and Date Range Selection:** Allows users to filter traces based on various criteria (properties) and select a specific date range to narrow down the displayed data.\n*   **Test Account Filtering:** Provides an option to filter out test accounts, ensuring that only relevant production data is displayed.\n*   **Trace ID Linking:** The ID column provides a link to a detailed view of each trace, allowing users to investigate specific requests further. The ID is shortened for readability.\n*   **Trace Name Linking:** The Trace Name column provides a link to a detailed view of each trace, allowing users to investigate specific requests further.\n*   **Timestamp Display:** Displays the timestamp of each trace, formatted with timezone information.\n*   **Latency Display:** Shows the latency of each trace in seconds.\n*   **Token Usage Display:** Displays the token usage for each trace, formatted for readability.\n*   **Cost Display:** Shows the total cost associated with each trace, formatted as currency.\n*   **Customizable Columns:** The `DataTable` component allows for customization of the displayed columns and their rendering.\n*   **Empty State Handling:** Displays a user-friendly message when no traces are found within the selected date range and filters.\n"
    },
    {
        "path": "products/llm_observability/frontend/ConversationDisplay/ConversationMessagesDisplay.tsx",
        "summary": "## Code Summary\n\nThe code defines React components for displaying conversations with Large Language Models (LLMs). The main component, `ConversationMessagesDisplay`, takes the input and output of an LLM interaction and renders them in a structured format. It normalizes the input and output messages, handles errors, and displays them using the `LLMInputOutput` component. The `LLMMessageDisplay` component is responsible for rendering individual messages, handling different content types (JSON, Markdown, plain text, images), and providing options to toggle the visibility and markdown rendering of the message content. It also displays any additional keyword arguments associated with the message as a JSON viewer.\n\n## Product Features\n\nHere's a breakdown of the features provided by the code:\n\n*   **Conversation Display:** Renders LLM conversations in a structured and readable format, separating input and output.\n*   **Input/Output Normalization:** Normalizes input and output messages to a consistent format for display.\n*   **Error Handling:** Displays error messages from the LLM, including the HTTP status code, in a clear and concise manner.\n*   **Message Content Rendering:** Handles different content types within messages, including:\n    *   JSON: Renders JSON content using a JSON viewer component.\n    *   Markdown: Renders Markdown content using a Markdown component, with an option to toggle rendering.\n    *   Plain Text: Renders plain text content.\n    *   Images: Renders images from URLs.\n*   **Message Visibility Toggle:** Allows users to toggle the visibility of individual messages.\n*   **Copy to Clipboard:** Provides a button to copy the message content to the clipboard.\n*   **Additional Keyword Arguments Display:** Displays any additional keyword arguments associated with a message as a JSON viewer.\n*   **Visual Spacing:** Adds visual spacing between input messages for better readability.\n*   **Theming:** Uses CSS variables for theming, allowing for customization of the appearance.\n"
    },
    {
        "path": "products/llm_observability/frontend/ConversationDisplay/ParametersHeader.tsx",
        "summary": "## Code Summary\n\nThe code defines a React component called `ParametersHeader`. This component takes an object `eventProperties` as a prop, specifically looking for a nested object called `$ai_model_parameters` within it. It then iterates through the key-value pairs in `$ai_model_parameters` and renders each pair as a `LemonTag` component, displaying the key and value. The `LemonTag` components are arranged in a flexible row with wrapping and a gap between them. It filters out null values.\n\n## Product Features\n\nBased on the code, the product likely has the following features related to LLM observability:\n\n*   **Display of LLM Model Parameters:** The code extracts and displays parameters associated with an AI model. This allows users to see the specific configurations used when the model was invoked.\n\n    *   **Description:** Shows the parameters used when calling an LLM, such as temperature, top\\_p, etc. This helps in understanding the context of the LLM interaction and debugging any issues.\n\n*   **Key-Value Pair Representation:** The parameters are displayed as key-value pairs, making it easy to understand the parameter name and its corresponding value.\n\n    *   **Description:** Presents the LLM parameters in a clear and organized format, improving readability and comprehension.\n\n*   **Filtering of Null Values:** The component filters out parameters with null values, preventing the display of irrelevant or missing information.\n\n    *   **Description:** Ensures that only relevant and meaningful parameters are displayed, reducing clutter and improving the user experience.\n\n*   **Tag-Based Display:** Uses `LemonTag` components to visually represent each parameter, providing a clear and distinct presentation.\n\n    *   **Description:** Uses visually distinct tags to represent each parameter, improving readability and making it easier to scan and identify specific parameters.\n"
    },
    {
        "path": "products/llm_observability/frontend/LLMObservabilityScene.tsx",
        "summary": "## Code Summary\n\nThe code defines the main component `LLMObservabilityScene` for the LLM Observability feature in PostHog. It provides a user interface for monitoring and analyzing LLM (Large Language Model) generations, traces, and users. The scene consists of several tabs: Dashboard, Traces, Generations, and Users. It uses Lemon UI components for layout, filtering, and data display. The scene also includes logic for handling date ranges, property filters, and test account filters. It checks for the presence of LLM generation events and prompts the user to instrument their LLM calls if none are detected.\n\n## Product Features\n\nHere's a breakdown of the features offered by the LLM Observability product, based on the code:\n\n*   **Dashboard:** Provides an overview of LLM activity through customizable tiles displaying key metrics and insights.\n*   **Traces:** Allows users to explore individual LLM traces, providing detailed information about the execution flow of LLM calls.\n*   **Generations:** Displays a table of LLM generations, allowing users to filter and sort based on properties and time ranges.\n*   **Users:** Provides insights into users interacting with LLMs.\n*   **Date Filtering:** Enables users to filter data based on specific date ranges.\n*   **Property Filtering:** Allows users to filter data based on custom properties associated with LLM generations.\n*   **Test Account Filtering:** Provides the option to exclude test accounts from the displayed data.\n*   **Data Table:** Displays LLM generation data in a tabular format with customizable columns and rendering.\n*   **Instrumentation Guidance:** Prompts users to instrument their LLM calls with the PostHog SDK if no LLM generation events are detected.\n*   **Trace ID Linking:** Links LLM generations to their corresponding traces for easy navigation and analysis.\n*   **Documentation Link:** Provides a direct link to the LLM Observability documentation.\n*   **Automatic Reload:** Allows users to manually trigger a data reload.\n"
    },
    {
        "path": "products/llm_observability/frontend/LLMObservabilityTraceScene.tsx",
        "summary": "## Code Summary\n\nThis code defines the `LLMObservabilityTraceScene` component, which is a scene in a PostHog application for visualizing and analyzing traces of Large Language Model (LLM) activity. It fetches and displays detailed information about a specific LLM trace, including its metadata, a tree-like representation of events within the trace, and the content of individual events. The scene utilizes several Lemon UI components for layout, styling, and data presentation. It also leverages Kea logic for state management and data fetching.\n\n## Product Features\n\nHere's a breakdown of the features provided by the `LLMObservabilityTraceScene` component:\n\n*   **Trace Visualization:** Displays a hierarchical tree structure of events within an LLM trace, allowing users to navigate and understand the flow of execution.\n*   **Event Details:** Shows detailed information about individual events within the trace, including input, output, metadata (e.g., tokens, cost, latency), and parameters.\n*   **Metadata Display:** Presents key metadata associated with the trace, such as the associated person, usage statistics, cost breakdown (input, output, total), and any associated metrics or feedback.\n*   **Error Highlighting:** Highlights error events within the trace, making it easy to identify potential issues.\n*   **Session Recording Integration:** Provides a link to view the session recording associated with the trace, enabling users to understand the context in which the LLM activity occurred.\n*   **Metrics and Feedback Display:** Shows metrics and user feedback associated with the trace in a table format.\n*   **Loading and Error Handling:** Displays a loading spinner while data is being fetched and provides error messages if there are issues.\n*   **LLM Input/Output Display:** Displays the input and output of LLM events in a structured format, using a JSON viewer for complex objects.\n*   **Conversation View:** For `$ai_generation` events, displays the conversation messages in a structured format, including tools, input, output, HTTP status, and error information.\n*   **Filtering and Navigation:** Allows users to select specific events in the trace tree to view their details.\n*   **Cost Analysis:** Displays the cost associated with the trace, including input cost, output cost, and total cost.\n*   **Latency Analysis:** Displays the latency of individual events and the overall trace.\n*   **Usage Tracking:** Displays the usage of the LLM, such as the number of tokens used.\n"
    },
    {
        "path": "products/llm_observability/frontend/types.ts",
        "summary": "This code defines TypeScript interfaces for representing messages and tool calls in Large Language Model (LLM) interactions, specifically focusing on OpenAI and Anthropic models, and also including some Vercel SDK message types. These interfaces are likely used for structuring data related to LLM inputs, outputs, and intermediate steps like tool usage. The `CompatMessage` and `CompatToolCall` interfaces suggest a compatibility layer or a unified format for handling different LLM providers.\n\nHere's a breakdown of the features implied by these types:\n\n**Features:**\n\n*   **Role-Based Messages:** Supports messages with defined roles (e.g., \"user\", \"assistant\", \"system\"), enabling structured conversations with LLMs.\n    *   Description: Allows for clear identification of the message sender and their role in the conversation, crucial for managing context and turn-taking.\n\n*   **OpenAI Tool Call Support:** Enables LLMs to call external tools and functions during the completion process.\n    *   Description: Provides a mechanism for LLMs to interact with external systems, expanding their capabilities beyond text generation. Includes support for tool call IDs.\n\n*   **Anthropic Message Format Support:**  Handles Anthropic's specific message formats, including text, tool calls, and tool results.\n    *   Description: Ensures compatibility with Anthropic's LLM API, allowing for proper interpretation and processing of Anthropic model outputs.\n\n*   **Vercel SDK Message Format Support:** Handles Vercel SDK message formats, including text and images.\n    *   Description: Ensures compatibility with Vercel SDK, allowing for proper interpretation and processing of Vercel model outputs.\n\n*   **Unified Message Handling (CompatMessage):** Provides a common interface for handling messages from different LLM providers.\n    *   Description: Simplifies the integration of multiple LLM providers by abstracting away their specific message formats.\n\n*   **Tool Result Handling:** Supports the processing of results returned by tools called by LLMs.\n    *   Description: Enables LLMs to incorporate the output of external tools into their responses, creating more informed and dynamic interactions.\n\n*   **Input and Completion Message Types:** Distinguishes between input messages (sent to the LLM) and completion messages (received from the LLM).\n    *   Description: Provides a clear separation between the data sent to the LLM and the data received back, facilitating data processing and analysis.\n"
    },
    {
        "path": "products/llm_observability/frontend/llmObservabilityLogic.tsx",
        "summary": "## Summary\n\nThis code defines the `llmObservabilityLogic` using Kea, a state management library. This logic manages the state and behavior of the LLM Observability feature in PostHog. It handles date filters, property filters, test account filtering, and queries for displaying data related to LLM generations and traces. It also defines the queries and structure for the dashboard tiles. The logic interacts with the PostHog API to fetch event definitions and uses the router to manage navigation and URL parameters.\n\n## Features\n\nHere's a breakdown of the features managed by this logic:\n\n*   **Dashboard:**\n    *   Displays key metrics related to LLM usage, including traces, generative AI users, total cost, cost per user, cost by model, generation calls, generation latency by model, and generations by HTTP status.\n    *   Allows users to filter data by date range, properties, and test accounts.\n    *   Provides drill-down functionality to navigate to the Generations or Traces views based on data point clicks.\n*   **Generations View:**\n    *   Displays a table of individual LLM generation events with details like model, latency, token usage, and cost.\n    *   Allows users to filter, search, and configure columns in the table.\n*   **Traces View:**\n    *   Displays a table of LLM traces, providing an overview of the end-to-end execution flow.\n    *   Allows users to filter, search, and export trace data.\n*   **Users View:**\n    *   Displays a table of users who have used LLM features, along with metrics like traces, generations, total cost, first seen, and last seen.\n    *   Allows users to filter, search, and configure columns in the table.\n*   **Filtering:**\n    *   Supports filtering by date range, event properties, person properties, groups, cohorts, and HogQL expressions.\n    *   Allows filtering out test accounts.\n*   **Data Refresh:**\n    *   Provides a mechanism to refresh the data displayed in the dashboard tiles and tables.\n    *   Indicates loading state during data refresh.\n*   **URL Management:**\n    *   Synchronizes the state of the LLM Observability feature with the URL, allowing users to share and bookmark specific views and filters.\n*   **AI Event Definition Check:**\n    *   Checks for the existence of the `$ai_generation` event definition to ensure proper data collection.\n"
    },
    {
        "path": "products/llm_observability/frontend/utils.ts",
        "summary": "This file provides utility functions for PostHog's LLM Observability feature. These functions are primarily focused on formatting, parsing, and normalizing data related to LLM traces and events, making it easier to display and analyze LLM interactions within PostHog. It handles data from various LLM providers like OpenAI, Anthropic, and Vercel SDK.\n\nHere's a breakdown of the features provided by the code:\n\n**Features:**\n\n*   **LLM Usage Formatting:**\n    *   `formatLLMUsage(trace_or_event)`: Formats the input and output tokens used by an LLM, displaying them as \"input → output (∑ total)\". It handles both `LLMTrace` and `LLMTraceEvent` types.\n*   **LLM Latency Formatting:**\n    *   `formatLLMLatency(latency)`: Formats the latency of an LLM interaction in seconds, rounding to two decimal places.\n*   **LLM Cost Formatting:**\n    *   `formatLLMCost(cost)`: Formats the cost of an LLM interaction in USD, using a currency formatter with a maximum of 4 decimal places.\n*   **LLM Trace/Event Type Detection:**\n    *   `isLLMTraceEvent(item)`: Determines if an item is an `LLMTraceEvent` or an `LLMTrace`.\n*   **Session ID Handling:**\n    *   `hasSessionID(event)`: Checks if an LLM trace or event has a session ID.\n    *   `getSessionID(event)`: Extracts the session ID from an LLM trace or event.\n*   **OpenAI Tool Call Parsing:**\n    *   `isOpenAICompatToolCall(input)`: Type guard to check if an input is a valid OpenAI-compatible tool call.\n    *   `isOpenAICompatToolCallsArray(input)`: Type guard to check if an input is an array of OpenAI-compatible tool calls.\n    *   `parseOpenAIToolCalls(toolCalls)`: Parses the arguments of OpenAI tool calls, converting string arguments to JSON objects.\n*   **Message Type Guards:**\n    *   `isOpenAICompatMessage(output)`: Type guard to check if an output is an OpenAI-compatible message.\n    *   `isAnthropicTextMessage(output)`: Type guard to check if an output is an Anthropic text message.\n    *   `isAnthropicToolCallMessage(output)`: Type guard to check if an output is an Anthropic tool call message.\n    *   `isAnthropicToolResultMessage(output)`: Type guard to check if an output is an Anthropic tool result message.\n    *   `isAnthropicRoleBasedMessage(input)`: Type guard to check if an input is an Anthropic role-based message.\n    *   `isVercelSDKTextMessage(input)`: Type guard to check if an input is a Vercel SDK text message.\n    *   `isVercelSDKImageMessage(input)`: Type guard to check if an input is a Vercel SDK image message.\n*   **Message Normalization:**\n    *   `normalizeMessage(output, defaultRole)`: Normalizes messages from different LLM providers (OpenAI, Anthropic, Vercel SDK) into a common format (`CompatMessage`) for consistent handling. It handles different message types, including text, tool calls, and tool results.\n    *   `normalizeMessages(messages, defaultRole, tools)`: Normalizes a list of messages, handling different message structures and adding tool information if provided.\n*   **Timestamp Formatting:**\n    *   `removeMilliseconds(timestamp)`: Removes milliseconds from a timestamp string and formats it in UTC.\n*   **LLM Event Title Formatting:**\n    *   `formatLLMEventTitle(event)`: Formats the title of an LLM event or trace, using the model name or span name if available.\n"
    }
]
